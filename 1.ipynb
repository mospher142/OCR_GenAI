{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import fitz\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.morphology import skeletonize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract images from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_file, zoom=6):\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "    output_dir = f\"{pdf_name}_images\"\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Check if images already exist in the output folder\n",
    "    existing_files = [f for f in os.listdir(output_dir) if f.endswith(\".png\")]\n",
    "    if existing_files:\n",
    "        print(f\"Images already exist in '{output_dir}'. Skipping conversion.\")\n",
    "        return output_dir  # Always return the directory path\n",
    "    \n",
    "    # Open the PDF file and extract images\n",
    "    doc = fitz.open(pdf_file)\n",
    "    print(f\"Converting PDF '{pdf_file}' with {doc.page_count} pages into images...\")\n",
    "\n",
    "    mat = fitz.Matrix(zoom, zoom)\n",
    "    page_count = 0\n",
    "\n",
    "    for i in range(doc.page_count):\n",
    "        output_path = os.path.join(output_dir, f\"image_{i+1}.png\")\n",
    "        page = doc.load_page(i)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        pix.save(output_path)\n",
    "        page_count += 1\n",
    "        print(f\"Saved {output_path}\")\n",
    "\n",
    "    doc.close()\n",
    "    \n",
    "    print(f\"Converted {page_count} pages to images in '{output_dir}'.\")\n",
    "    return output_dir  # Return the output directory path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(folder_path):\n",
    "    for image_file in os.listdir(folder_path):\n",
    "        if image_file.endswith(\".png\") and image_file.startswith(\"image_\"):\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            \n",
    "            # Load image using cv2\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            # Determine the image height and width\n",
    "            height, width, _ = image.shape\n",
    "\n",
    "            # Crop logic\n",
    "            if \"image_1\" in image_file:  \n",
    "                top_crop = 900  # Crop more from the top for page 1\n",
    "                image = image[top_crop:, :]\n",
    "            else:\n",
    "                top_crop = 200  # Crop less from the top for other pages\n",
    "                image = image[top_crop:, :]\n",
    "            \n",
    "            bottom_crop = 300  \n",
    "            image = image[:-bottom_crop, :]\n",
    "\n",
    "            # Left crop logic\n",
    "            left_crop = 600  \n",
    "            image = image[:, left_crop:]  \n",
    "            \n",
    "            # Right \n",
    "            right_crop = 200  \n",
    "            image = image[:, :-right_crop]  \n",
    "\n",
    "            ## Convert to grayscale and denoise\n",
    "            gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            denoised_img = cv2.GaussianBlur(gray_img, (5, 5), 1)\n",
    "\n",
    "            # Adjust contrast\n",
    "            #adjusted = cv2.convertScaleAbs(denoised_img, alpha=2.0, beta=1)\n",
    "\n",
    "            _, im_bw = cv2.threshold(denoised_img, 250, 150, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "            # Morphological operations\n",
    "            #kernel = np.ones((2, 2), np.uint8)\n",
    "            #opening = cv2.morphologyEx(im_bw, cv2.MORPH_OPEN, kernel)\n",
    "            kernel1 = np.ones((2, 3), np.uint8)\n",
    "            erosion = cv2.erode(im_bw, kernel1, iterations=1)\n",
    "\n",
    "            # Adaptive thresholding\n",
    "            \n",
    "\n",
    "            #kernel2 = np.ones((1, 1), np.uint8)\n",
    "            #dilation = cv2.dilate(erosion, kernel2, iterations=1)\n",
    "\n",
    "            # Optional scaling\n",
    "            #scale_percent = 200\n",
    "            #width = int(dilation.shape[1] * scale_percent / 100)\n",
    "            #height = int(dilation.shape[0] * scale_percent / 100)\n",
    "            #scaled_img = cv2.resize(dilation, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Save the processed image in the same folder with a new name\n",
    "            processed_image_path = os.path.join(folder_path, f\"processed_{image_file}\")\n",
    "\n",
    "            cv2.imwrite(processed_image_path, im_bw)\n",
    "            print(f\"Processed and saved {processed_image_path}\")\n",
    "\n",
    "    print(f\"All images in '{folder_path}' have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_with_easyocr(image_path, reader, output_folder):\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Detect text using EasyOCR\n",
    "    results = reader.readtext(img)\n",
    "    \n",
    "    # Initialize lists to store the coordinates of bounding boxes\n",
    "    horizontal_boxes = []\n",
    "    current_line_boxes = []\n",
    "\n",
    "    # Collect bounding boxes that are horizontal\n",
    "    for (bbox, text, confidence) in results:\n",
    "        if confidence > 0.2:  # Confidence threshold\n",
    "            (top_left, top_right, bottom_right, bottom_left) = bbox\n",
    "            top_left = tuple(map(int, top_left))\n",
    "            bottom_right = tuple(map(int, bottom_right))\n",
    "\n",
    "            # Calculate width and height\n",
    "            width = bottom_right[0] - top_left[0]\n",
    "            height = bottom_right[1] - top_left[1]\n",
    "\n",
    "            # Only process horizontal bounding boxes\n",
    "            if width > height:\n",
    "                horizontal_boxes.append((top_left[0], top_left[1], bottom_right[0], bottom_right[1]))\n",
    "\n",
    "    # Sort boxes by their y-coordinate for line-based grouping\n",
    "    horizontal_boxes.sort(key=lambda box: box[1])\n",
    "\n",
    "    # Merge bounding boxes on the same line\n",
    "    merged_boxes = []\n",
    "    for box in horizontal_boxes:\n",
    "        if not current_line_boxes:\n",
    "            current_line_boxes.append(box)\n",
    "            continue\n",
    "\n",
    "        _, y1, _, y2 = box\n",
    "        _, prev_y1, _, prev_y2 = current_line_boxes[-1]\n",
    "\n",
    "        # Check if the box is on the same line as the previous one\n",
    "        if abs(y1 - prev_y1) <= 10 or abs(y2 - prev_y2) <= 10:  # y_threshold\n",
    "            current_line_boxes.append(box)\n",
    "        else:\n",
    "            min_x1 = min(b[0] for b in current_line_boxes)\n",
    "            min_y1 = min(b[1] for b in current_line_boxes)\n",
    "            max_x2 = max(b[2] for b in current_line_boxes)\n",
    "            max_y2 = max(b[3] for b in current_line_boxes)\n",
    "            merged_boxes.append((min_x1, min_y1, max_x2 - min_x1, max_y2 - min_y1))\n",
    "            current_line_boxes = [box]\n",
    "\n",
    "    if current_line_boxes:\n",
    "        min_x1 = min(b[0] for b in current_line_boxes)\n",
    "        min_y1 = min(b[1] for b in current_line_boxes)\n",
    "        max_x2 = max(b[2] for b in current_line_boxes)\n",
    "        max_y2 = max(b[3] for b in current_line_boxes)\n",
    "        merged_boxes.append((min_x1, min_y1, max_x2 - min_x1, max_y2 - min_y1))\n",
    "\n",
    "    # Draw merged bounding boxes on the image\n",
    "    for (x, y, w, h) in merged_boxes:\n",
    "        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box\n",
    "\n",
    "    # Save the image with merged bounding boxes\n",
    "    output_image_path = os.path.join(output_folder, f\"bounding_boxes_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(output_image_path, img)\n",
    "    print(f\"Image with merged bounding boxes saved as {output_image_path}\")\n",
    "    \n",
    "    return merged_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from each bounding box in an image\n",
    "def extract_text_from_merged_boxes(merged_boxes, img, reader):\n",
    "    document = []  # List to store the extracted text from each bounding box\n",
    "    \n",
    "    # Perform OCR on each bounding box region\n",
    "    for i, (x, y, w, h) in enumerate(merged_boxes):\n",
    "        # Crop each region of interest\n",
    "        roi = img[y:y + h, x:x + w]\n",
    "\n",
    "        # Perform OCR on the cropped region\n",
    "        result = reader.readtext(roi, decoder='beamsearch', detail=1)\n",
    "\n",
    "        # Extract text from OCR results and append to document\n",
    "        box_text = \" \".join([text for (_, text, confidence) in result if confidence > 0.2])\n",
    "        document.append(box_text)\n",
    "\n",
    "    return document  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_extract_text_from_images(folder_path):\n",
    "    # Initialize the EasyOCR reader with the desired language\n",
    "    reader = easyocr.Reader(['fr'])  # Specify language\n",
    "    \n",
    "    # Create an output folder for images with bounding boxes if it doesn't exist\n",
    "    output_folder = os.path.join(folder_path, \"processed_with_boxes\")\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Dictionary to store the extracted text for each image\n",
    "    extracted_texts = {}\n",
    "\n",
    "    # Process each image that starts with \"processed_\"\n",
    "    for image_file in os.listdir(folder_path):\n",
    "        if image_file.startswith(\"processed_\") and image_file.endswith(\".png\"):\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            img = cv2.imread(image_path)\n",
    "            \n",
    "            # Step 1: Detect and merge bounding boxes (this will not save new images)\n",
    "            merged_boxes = process_image_with_easyocr(image_path, reader, output_folder)\n",
    "\n",
    "            # Step 2: Extract text from each bounding box\n",
    "            document = []  # List to store text extracted from each bounding box\n",
    "            for (x, y, w, h) in merged_boxes:\n",
    "                # Crop each region of interest\n",
    "                roi = img[y:y + h, x:x + w]\n",
    "                \n",
    "                # Perform OCR on the cropped region\n",
    "                result = reader.readtext(roi, detail=1)\n",
    "                \n",
    "                # Extract text from OCR results and append to document\n",
    "                box_text = \" \".join([text for (_, text, confidence) in result if confidence > 0.2])\n",
    "                document.append(box_text)\n",
    "            \n",
    "            # Store the extracted text for each image\n",
    "            extracted_texts[image_file] = document\n",
    "\n",
    "    # Save the extracted text from all images into a single .txt file\n",
    "    output_text_file = os.path.join(folder_path, \"extracted_texts.txt\")\n",
    "    with open(output_text_file, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        for image_name, texts in extracted_texts.items():\n",
    "            text_file.write(f\"### {image_name} ###\\n\")\n",
    "            text_file.write(\"\\n\".join(texts) + \"\\n\\n\")\n",
    "    \n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already exist in '24000005_images'. Skipping conversion.\n",
      "Processed and saved 24000005_images\\processed_image_1.png\n",
      "All images in '24000005_images' have been processed.\n",
      "Image with merged bounding boxes saved as 24000005_images\\processed_with_boxes\\bounding_boxes_processed_image_1.png\n",
      "{'processed_image_1.png': ['24000005* du tribuffâl dë lentreprise greffe au', 'au greffe', '', 'Nofn', '(en entler) : GRANDR', '(en abrégé)', 'Forme légale 2 SRL', 'Adresse complèle du siège 209 Boulevard Lambermont; Bolte 12, 1030 Schaerbeok', 'MODIFICATION SIEGE SOCIAL', \"résulle dune décision émise par /organe d'adminietratlon; reçu par Tresor Madoda Tuyinama; gérant de\", 'la soclété GRANDR SRL le 13 décombre 2023 , contenant procos-verbal do [assemblée générale extraordinaire', 'W de la soc8té à responsablllte llmitée GRANDR ayant son slège soclal à 1030 Schaerbeek; Boulevard', 'Lambermont 209/b012 que:', 'Premlèra réxoluton Modifcation do ladresse du sluge social de la sociéte_', \"Lorgane dadmlnistration a informé l'assemblee générale extraordinaire du 13 décembre 2023 que l'edresse\", '', '1050 Ixelles.', 'Deuxiàmne régolutlon Approbation de la modlcation présentée', 'Lassemblée  génénérale  extraordinalre du 13 décembre 2023,accepte et   confilne le changement', 'dadrease du sl8ge socal vors le 54 avenue Louise; 1050 Ixelles.', 'Trolsiemne régoluton Coordlnatlon 0t publication', \"L'assemblée générale extraordinaire déclde do modlfier ['adresse du slège social confonmément à co qui est\", 'dit c-avant', \"L'assemblée générale extraordinalre donne lous pouvoirs à Fadminstrateur pour faire le nécéssalre quant à\", 'la modificaton à eflectuer ainsl que sa publication au moniteur et dans tous autre reglstre légal où cela serait', 'nécessalre;', 'POUR EXTRAIT ANALYTIQUE', 'Toutes cos résolutions sont prises à (unanimilé.', 'Gérant Trescr MADODA TUYINAMA', \"Déposé: Une @xpedltion du rapport de I'orgene dadminstratlon contenant PV de lassemblée générale\", 'extraordinalre du 13 décembre 2023.', 'sur la dernlere page du VoletB : Nom 8t quallto du notaire Instrumentant ou do la porgonnoou dom pereonnos', \"ayant pouvoir do ropresenter {a personno morale à l'éqard do8 {iors\", 'Nom et slgnature (pas applicable aux acte8 de type & Mantlon %)']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 6):\n",
    "    pdf_file = f\"extract/240000{i:02}.pdf\"  \n",
    "    output_dir = pdf_to_images(pdf_file)\n",
    "    preprocess_images(output_dir)\n",
    "    \n",
    "    extracted_texts = process_and_extract_text_from_images(output_dir)\n",
    "    print(extracted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
